{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信用情報の分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信用情報\n",
    "\n",
    "Kaggleの Home Credit Default Risk コンペティションを活用し、実データに近いものに対し、自ら課題を設定して分析する練習を行います。<br>\n",
    "\n",
    "Home Credit Default Risk | Kaggle<br>\n",
    "\n",
    "Week4では機械学習手法を用いて学習・推定を行います。その準備としてWeek3でデータ探索（EDA）を行います。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題1】コンペティション内容の把握<br>\n",
    "コンペティションのOverviewページ読み、「Home Credit Default Risk」はどのようなコンペティションか、以下の観点からレポートしてください。<br>\n",
    "\n",
    "Home Creditはどのような企業？<br>\n",
    "このコンペティションでは何を予測する？<br>\n",
    "それを予測することで企業が得られるメリットは何？<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "train = pd.read_csv('./Home_credit_Default_Risk/application_train.csv')\n",
    "pd.set_option('display.max_columns', train.shape[1])\n",
    "pd.set_option('display.max_rows', 100)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Home Creditはどのような企業？<br>\n",
    "Home Credit社は世界的に銀行口座を持たないような顧客に対する小口ローン融資を9ヵ国で展開している消費者金融。<br>\n",
    "顧客の購買履歴に基づく融資を行っている。<br>\n",
    "2.このコンペティションでは何を予測する？<br>\n",
    "顧客の返済可・不可を予測する<br>\n",
    "3.それを予測することで企業が得られるメリットは何？<br>\n",
    "適切な融資額を決定することができる為、貸し倒れリスクを減らすことができる。<br>\n",
    "\n",
    "今回のような2値分類を行うタスクの場合のEDAの心構え<br>\n",
    "与えられてた訓練データから、データが所属するクラス毎に、どんな特徴が存在するのかを特徴量から見出し、新規の特徴量を作成したり、<br>\n",
    "似た特徴量を減らしたり、くっつけて新しい特徴量を生み出したりすること<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】データの概観の把握<br>\n",
    "データの概観を把握するために以下のようなことを行ってください。<br>\n",
    "\n",
    "・.head()、.info()、describe()などを使う<br>\n",
    "・欠損値の有無を見る<br>\n",
    "・クラスの割合がわかるグラフを描く<br>\n",
    "\n",
    "それぞれ結果に対する説明や考察も行ってください。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .head()、.info()、describe()などを使う\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値の有無を見る\n",
    "n_missing = train.isnull().sum() \n",
    "print(\"欠損数が{}より多い特徴量の一覧\\n{}\".format(0, n_missing[n_missing>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データタイプ毎に特徴量を分割\n",
    "numerical_feats = train.dtypes[train.dtypes != 'object'].index\n",
    "categorical_feats = train.dtypes[train.dtypes == 'object'].index\n",
    "\n",
    "print(numerical_feats, len(numerical_feats))\n",
    "print(categorical_feats, len(categorical_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値データだけ表示\n",
    "train[numerical_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリーデータだけ表示\n",
    "train[categorical_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスの割合がわかるグラフを描く\n",
    "train.TARGET.value_counts().plot(kind='pie', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ全体の返済完了者、未完了者の割合を確認\n",
    "col = 'TARGET'\n",
    "temp = train[col].value_counts()\n",
    "df = pd.DataFrame({'labels': temp.index,\n",
    "                   'values': temp.values\n",
    "                  })\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.title('Application loans repayed - Pay_ok[{}]'.format(col))\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x = 'labels', y=\"values\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年収ベースで見たときの外れ値の確認\n",
    "thresh = 10000000\n",
    "train[train['AMT_INCOME_TOTAL'] > thresh]\n",
    "# income_totalの外れ値　こいつは外していいのでは？\n",
    "# 年収1億ドルで労働者で返済できませんでした。は明らかにおかしいし、そもそも何故借金の必要が？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考察：<br>\n",
    "0が、支払いに問題ない人、1が、支払いに問題がある（デフォルト）人で、その割合が0に偏ってしまっている。<br>\n",
    "そこで、1に分類されている人たちの中の共通点を探してあげる必要があると考える。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題3】課題設定<br>\n",
    "データの概観を見たことを元に、自分なりの課題・疑問を複数設定してください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.target:0とtarget:1での年収の分布を見る<br>\n",
    "2.貸し倒れてしまう人の要因<br>\n",
    "3.貸倒ない人の行動、要因は？<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4】データ探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返済できている人、そうでない人との間に年収において差があるのではないかと考えたので、<br>\n",
    "データを分けて分布を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_ok = train[train['TARGET'] == 0]\n",
    "default = train[train['TARGET'] == 1]\n",
    "\n",
    "sns.distplot(pay_ok['AMT_INCOME_TOTAL'], bins=500, fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(pay_ok['AMT_INCOME_TOTAL'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値が極端に寄ってしまっていて見づらいので対数変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返済完了者データ：対数変換後\n",
    "sns.distplot(pay_ok['AMT_INCOME_TOTAL'].apply(np.log), bins=100, fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(pay_ok['AMT_INCOME_TOTAL'].apply(np.log), plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返済完了者データ：対数変換前\n",
    "sns.distplot(default['AMT_INCOME_TOTAL'], bins=500, fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(default['AMT_INCOME_TOTAL'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらも同様"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返済未完了者データ：対数変換後\n",
    "sns.distplot(default['AMT_INCOME_TOTAL'].apply(np.log), bins=500, fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(default['AMT_INCOME_TOTAL'].apply(np.log), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QQプロット的には、年収はで正規性が確保できていると言えるので、予測には使えそう。<br>\n",
    "同じヒストグラムで分布を比較してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pay_ok['AMT_INCOME_TOTAL'].apply(np.log)\n",
    "b = default['AMT_INCOME_TOTAL'].apply(np.log)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist([a, b], bins=100, label=['ok', 'default'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返済完了できている人とそうでない人との分布にそこまでの差はなさそう。<br>\n",
    "もしかしたら年収単体では意味をなさないかも？<br>\n",
    "\n",
    "データにyes,noや0, 1のデータが多いので、barplotで返済できている人、そうでない人に特徴がないか確認してみる<br>\n",
    "まずは融資方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"NAME_CONTRACT_TYPE\"\n",
    "temp = pay_ok[col].value_counts()\n",
    "df = pd.DataFrame({'labels': temp.index,\n",
    "                   'values': temp.values\n",
    "                  })\n",
    "\n",
    "temp2 = default[col].value_counts()\n",
    "df2 = pd.DataFrame({'labels': temp2.index,\n",
    "                   'values': temp2.values\n",
    "                  })\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.title('Application loans repayed - Pay_ok[{}]'.format(col))\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x = 'labels', y=\"values\", data=df)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.title('Application loans repayed - default[{}]'.format(col))\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x = 'labels', y=\"values\", data=df2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df['CONTRACT_TYPE_ratio'] = df['values'] / df['values'].sum()\n",
    "df2['CONTRACT_TYPE_ratio'] = df2['values'] / df2['values'].sum()\n",
    "\n",
    "print(df)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比率を見てみると、返済できている人にも、そうでない人にもリボルビングの人はいるし、キャッシュローンの人も一定数存在している<br>\n",
    "あまり有意ではなさそう。<br>\n",
    "<br>\n",
    "その他のカテゴリーデータでどんな感じか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_feats:\n",
    "    temp = pay_ok[col].value_counts()\n",
    "    df = pd.DataFrame({'labels': temp.index,\n",
    "                       'values': temp.values\n",
    "                      })\n",
    "    temp = default[col].value_counts()\n",
    "    df2 = pd.DataFrame({'labels': temp.index,\n",
    "                       'values': temp.values\n",
    "                      })\n",
    "    plt.subplot(1, 2, 1)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "    plt.title('Pay_ok[{}]'.format(col))\n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(x = 'labels', y=\"values\", data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "    plt.title('default[{}]'.format(col))\n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(x = 'labels', y=\"values\", data=df2)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    df[col + '_ratio'] = df['values'] / df['values'].sum()\n",
    "    df2[col + '_ratio'] = df2['values'] / df2['values'].sum()\n",
    "    \n",
    "    print('#'*50)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(df)\n",
    "    print('-'*50)\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリーデータにおける欠損値は現時点で未処理ではあるものの、返済できている人、そうでない人との間で特徴の偏りが見られなかった。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値データで、特徴量ごとの分布がどうなっているかを見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_feats:\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Pay_ok[{}]'.format(col))\n",
    "#     sns.set_color_codes(\"pastel\")\n",
    "#     sns.distplot(pay_ok[col], bins=500, fit=norm)\n",
    "    pay_ok[col].hist(bins=100)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "    plt.title('default[{}]'.format(col))\n",
    "#     sns.set_color_codes(\"pastel\")\n",
    "#     sns.distplot(default[col], bins=500, fit=norm)\n",
    "    default[col].hist(bins=100)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([])\n",
    "a['credit_ratio'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "plt.xlim(0, 25)\n",
    "plt.hist(a['credit_ratio'], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借入日から何日前に生まれたかと言うDAY_BIRTHの項目で、返済できている人とそうでない人の分布に明確な違いがあった。比較的若者に、返済能力が低い傾向が見られる。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_ok[pay_ok['DAYS_BIRTH'] < 15000]\n",
    "a = pay_ok.copy()\n",
    "a['DAYS_BIRTH'] = abs(a['DAYS_BIRTH'] / 365)\n",
    "a['DAYS_BIRTH'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_old_are_you = default.copy()\n",
    "how_old_are_you['DAYS_BIRTH'] = abs(how_old_are_you['DAYS_BIRTH'] / 365)\n",
    "how_old_are_you['DAYS_BIRTH'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のタスクが0,1の予測なので、目的変数と説明変数間の相関に意味はないと思うが、説明変数間での相関がどうなっているのかを確認する。<br>\n",
    "事前課題で作成した関数をもとに、返済できている人、そうでない人ごとに特徴量の相関がないかチェック。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_corr = train[train.TARGET == 0].corr()\n",
    "default_corr = train[train['TARGET'] == 1].corr()\n",
    "\n",
    "# 選んだ10個の特徴量の中でお互いの相関係数が高い組み合わせを3つ探し出す。\n",
    "def calc_corr(df):\n",
    "    # 相関係数行列を作成\n",
    "    corr_mat = df.corr(method='pearson')\n",
    "\n",
    "    # 行（列）サイズを取得\n",
    "    n = corr_mat.shape[0]\n",
    "\n",
    "    # 項目名を取得\n",
    "    columns = corr_mat.columns.tolist()\n",
    "\n",
    "    # 変数名1, 変数名2, 値を一つの配列に入れたものを作成\n",
    "    # 相関係数行列の下三角部分（対角成分除く）だけ\n",
    "    corr_ary = []\n",
    "    var1_ary = []\n",
    "    var2_ary = []\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "#             print(i, j)\n",
    "            if i == j:\n",
    "                continue\n",
    "            corr_ary.append(corr_mat.iloc[i,j])\n",
    "            var1_ary.append(columns[i])\n",
    "            var2_ary.append(columns[j])\n",
    "\n",
    "    # dfにする\n",
    "    df_new = pd.DataFrame([])\n",
    "    df_new[\"var1\"] = var1_ary\n",
    "    df_new[\"var2\"] = var2_ary\n",
    "    df_new[\"corr\"] = corr_ary\n",
    "\n",
    "    return df_new\n",
    "\n",
    "ok = calc_corr(ok_corr)\n",
    "ok.sort_values('corr', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = calc_corr(default_corr)\n",
    "no.sort_values('corr', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだけだと相関は現時点で使い物にならなさそう。<br>\n",
    "適切な相関値が求まっているとは思えない。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下個人的な調査学習\n",
    "## Home Credit Default Risk 1位ソリューション\n",
    "### null importanceによる特徴量選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Selection with NUll Importance<br>\n",
    "URL:https://www.kaggle.com/ogrellier/feature-selection-with-null-importances<br>\n",
    "null importanceについての解説記事<br>\n",
    "https://qiita.com/trapi/items/1d6ede5d492d1a9dc3c9<br>\n",
    "2位のソリューション：https://github.com/KazukiOnodera/Home-Credit-Default-Risk<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Home_credit_Default_Risk/application_train.csv')\n",
    "\n",
    "categoricaal_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "categorical_feats\n",
    "for f_ in categorical_feats:\n",
    "    data[f_], _ = pd.factorize(data[f_])\n",
    "    # Set feature type as categorical\n",
    "    data[f_] = data[f_].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(data, shuffle, seed=None):\n",
    "    # Gather real features\n",
    "    train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']]\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "    # Shuffle target if required\n",
    "    y = data['TARGET'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['TARGET'].copy().sample(frac=1.0)\n",
    "    \n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': seed,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n",
    "    \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gain:評価基準をどれだけ改善させたることができる(できた)のかという値<br>\n",
    "split:ツリーの分割にその特徴量が使われた回数をカウントしたもの<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df = pd.DataFrame()\n",
    "nb_runs = 80\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=data, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    # Plot Split importances\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n",
    "    # Plot Gain importances\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())\n",
    "    \n",
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df.to_csv('null_importances_distribution_rf.csv')\n",
    "actual_imp_df.to_csv('actual_importances_ditribution_rf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関係のない特徴量を削除した場合の影響度の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    correlation_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\n",
    "fig.subplots_adjust(top=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n",
    "    # Fit LightGBM \n",
    "    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': .1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 13,\n",
    "        'n_jobs': 4,\n",
    "        'min_split_gain': .00001,\n",
    "        'reg_alpha': .00001,\n",
    "        'reg_lambda': .00001,\n",
    "        'metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    hist = lgb.cv(\n",
    "        params=lgb_params, \n",
    "        train_set=dtrain, \n",
    "        num_boost_round=2000,\n",
    "        categorical_feature=cat_feats,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=0,\n",
    "        seed=17\n",
    "    )\n",
    "    # Return the last mean / std values \n",
    "    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n",
    "\n",
    "# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]\n",
    "# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])\n",
    "\n",
    "for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]:\n",
    "    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n",
    "    split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n",
    "    gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "                                                                                             \n",
    "    print('Results for threshold %3d' % threshold)\n",
    "    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET'])\n",
    "    print('\\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1]))\n",
    "    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET'])\n",
    "    print('\\t GAIN  : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
