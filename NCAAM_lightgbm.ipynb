{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータチューニング時に参考にしたサイト：<br>\n",
    "https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f<br>\n",
    "https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html<br>\n",
    "https://hackerdemy.com/2020/09/15/lightgbm-classification/<br>\n",
    "https://zenn.dev/mosamosa/articles/07d0076c9292136a3639<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as lgbo\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DIR_PATH = '../kaggle/MDataFiles_Stage2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DIR_PATH+'/final_regular_result.csv')\n",
    "test = pd.read_csv(DIR_PATH+'/stage1_test.csv')\n",
    "\n",
    "y = train[\"result\"]\n",
    "s = train[\"Season\"]\n",
    "X = train.drop(['Season','TeamID1','TeamID2','result'], axis=1)\n",
    "X_test = test.drop(['ID', 'Season','TeamID1','TeamID2'], axis=1)\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot-encodingでtrain, testデータのカラムデータを合わせる\n",
    "x_column = X.columns\n",
    "test_column = X_test.columns\n",
    "no_train_feat = []\n",
    "no_test_feat = []\n",
    "\n",
    "for i in x_column:\n",
    "    if i not in test_column:\n",
    "        no_train_feat.append(i)\n",
    "        \n",
    "X.drop(no_train_feat, axis=1, inplace=True)\n",
    "\n",
    "x_column = X.columns\n",
    "test_column = X_test.columns\n",
    "for i in test_column:\n",
    "    if i not in x_column:\n",
    "        no_test_feat.append(i)\n",
    "X_test.drop(no_test_feat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用モデル：llightgbm<br>\n",
    "CV： groupKfold(n=4)<br>\n",
    "アンサンブル： CV毎にtestデータの確率予測を行い、nfoldで加重平均を行う<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング：https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f\n",
    "# -------------------現状のベストパラメータ保管場所------------------\n",
    "# 候補1 : cv mean : 0.4949, 使用特徴量：全て(2015~2021), kfold=6\n",
    "best_lgb_params1 = {'objective': 'binary',\n",
    "              'metric': 'binary_logloss',\n",
    "              'boosting': 'gbdt',\n",
    "              'num_leaves': 128,\n",
    "              'feature_fraction': 0.6,\n",
    "              'bagging_fraction': 0.6,\n",
    "              'bagging_freq': 5,\n",
    "              'learning_rate': 0.05,\n",
    "              'max_bin':600,\n",
    "              'seed':2021,\n",
    "                'num_iterations':10000\n",
    "}\n",
    "\n",
    "# 候補3: cv mean:0.4925, kfold=6\n",
    "best_lgb_params3 = {'objective': 'binary',\n",
    "              'metric': 'binary_logloss',\n",
    "              'boosting': 'gbdt',\n",
    "              'num_leaves': 1000,\n",
    "              'max_depth': 15,\n",
    "              'feature_fraction': 0.3,\n",
    "              'bagging_fraction': 0.6,\n",
    "              'bagging_freq': 5,\n",
    "              'learning_rate': 0.01,\n",
    "              'max_bin':125,\n",
    "              'seed':2021,\n",
    "              'min_data_in_leaf':13,\n",
    "              'lambda_l1': 2.5,\n",
    "              'extra_trees': True,\n",
    "              'num_iterations':10000\n",
    "}\n",
    "\n",
    "# -------------------色々パラメータで遊んで精度見てみる場所------------------\n",
    "# lgb_params = {'objective': 'binary',\n",
    "#               'metric': 'binary_logloss',\n",
    "#               'boosting': 'gbdt',\n",
    "#               'num_leaves': 23,\n",
    "#               'max_depth' : 5,\n",
    "#               'max_bin' : 600,\n",
    "#               'min_sum_hessian_in_leaf' : 1e-5,\n",
    "#               'feature_fraction': 0.8,\n",
    "#               'bagging_fraction': 0.6,\n",
    "#               'bagging_freq': 5,\n",
    "#               'learning_rate': 0.05,\n",
    "#               'num_iterations' : 5000,\n",
    "#               'random_state' : 42\n",
    "#              }\n",
    "\n",
    "# cv mean:0.492→特徴量全乗せ、2019までののシーズン 　0.4813→全乗せ、全てのシーズン\n",
    "# cv mean:→特徴量全盛、2015~2021使用\n",
    "# lgb_params = {'objective': 'binary',\n",
    "#               'metric': 'binary_logloss',\n",
    "#               'boosting': 'gbdt',\n",
    "#               'num_leaves': 1000,\n",
    "#               'max_depth': 15,\n",
    "#               'feature_fraction': 0.1,\n",
    "#               'bagging_fraction': 0.6,\n",
    "#               'bagging_freq': 5,\n",
    "#               'learning_rate': 0.01,\n",
    "#               'max_bin':125,\n",
    "#               'seed':2021,\n",
    "#               'min_data_in_leaf':13,\n",
    "#               'lambda_l1': 2.5,\n",
    "#               'extra_trees': True,\n",
    "#               'num_iterations':10000\n",
    "# }\n",
    "\n",
    "lgb_params = {'objective': 'regression',\n",
    "              'metric': 'mae',\n",
    "              'boosting': 'dart',\n",
    "              'num_leaves': 32,\n",
    "              'min_data_in_leaf': 40,\n",
    "              'feature_fraction': 0.9,\n",
    "              'bagging_fraction': 0.8,\n",
    "              'bagging_freq': 3,\n",
    "              'learning_rate': 0.02,\n",
    "              'num_iterations' : 300,\n",
    "              'max_depth':5\n",
    "             }\n",
    "\n",
    "def model_training(X, y, cv, groups, params, metric, early_stopping=10, \\\n",
    "    plt_iter=True, X_test=[], cat_features=[]):\n",
    "\n",
    "    feature_importance = pd.DataFrame()\n",
    "    val_scores=[]\n",
    "    train_evals=[]\n",
    "    valid_evals=[]\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        test_pred=np.zeros(len(X_test))\n",
    "\n",
    "    for idx, (train_index, val_index) in enumerate(cv.split(X, y, groups)):\n",
    "\n",
    "        print(\"###### fold %d ######\" % (idx+1))\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "    \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                  early_stopping_rounds=early_stopping,\n",
    "                  verbose=20\n",
    "                  #categorical_feature=list(cate_ft_lst),\n",
    "                  )\n",
    "\n",
    "        val_scores.append(model.best_score_['valid_1'][metric])\n",
    "        train_evals.append(model.evals_result_['training'][metric])\n",
    "        valid_evals.append(model.evals_result_['valid_1'][metric])\n",
    "        \n",
    "\n",
    "        if len(X_test) > 0:\n",
    "            test_pred = test_pred + model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = X_train.columns\n",
    "        fold_importance[\"importance\"] = model.feature_importances_\n",
    "        fold_importance[\"fold\"] = idx+1\n",
    "        feature_importance = pd.concat([feature_importance, fold_importance]\n",
    "                                       , axis=0)\n",
    "    \n",
    "    \n",
    "    if plt_iter:\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 2, figsize=(9,20))\n",
    "        \n",
    "        for i, ax in enumerate(axs.flatten()):\n",
    "            ax.plot(train_evals[i], label='training')\n",
    "            ax.plot(valid_evals[i], label='validation')\n",
    "            ax.set(xlabel='interations', ylabel=f'{metric}')\n",
    "            ax.set_title(f'fold {i+1}', fontsize=12)\n",
    "            ax.legend(loc='upper right', prop={'size': 9})\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print('### CV scores by fold ###')\n",
    "    for i in range(cv.get_n_splits(X)):\n",
    "        print(f'fold {i+1}: {val_scores[i]:.4f}')\n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'\\\n",
    "          .format(np.mean(val_scores), np.std(val_scores)))\n",
    "    \n",
    "    feature_importance = feature_importance[[\"feature\", \"importance\"]]\\\n",
    "                         .groupby(\"feature\").mean().sort_values(\n",
    "                         by=\"importance\", ascending=False)\n",
    "    feature_importance.reset_index(inplace=True)\n",
    "\n",
    "    if len(X_test) > 0:\n",
    "        test_pred = test_pred / cv.get_n_splits(X)\n",
    "        return feature_importance, test_pred\n",
    "    else:\n",
    "        return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・推定\n",
    "%%time\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "feature_importance, test_pred = \\\n",
    "    model_training(X, y, group_kfold, s, lgb_params, \n",
    "    'l1', early_stopping=50, plt_iter=True, X_test=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance出力\n",
    "plt.figure(figsize=(10, 10));\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance)\n",
    "plt.title('Feature Importnace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終予測結果出力\n",
    "MSampleSubmission = pd.read_csv(DIR_PATH+'/MSampleSubmissionStage2.csv')\n",
    "idx = test_pred.shape[0] // 2\n",
    "test_pred[idx:] = 1 - test_pred[idx:]\n",
    "\n",
    "pred = pd.concat([test.ID, pd.Series(test_pred)], axis=1).groupby('ID')[0]\\\n",
    "        .mean().reset_index().rename(columns={0:'Pred'})\n",
    "sub = MSampleSubmission.drop(['Pred'],axis=1).merge(pred, on='ID')\n",
    "sub.to_csv('small_mae_score.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果の分布確認\n",
    "sub['Pred'].hist(bins=100)\n",
    "print(len(sub.loc[sub['Pred'] > 0.5, 'Pred']))\n",
    "print(len(sub.loc[sub['Pred'] < 0.5, 'Pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
